# ğŸš€ Projeto de Pipeline de Dados 

Este projeto implementa um pipeline completo de dados com foco em ingestÃ£o, transformaÃ§Ã£o, carga e anÃ¡lise de dados financeiros. 
Dados sÃ£o manipulados e analisados para geraÃ§Ã£o de insights relevantes, sendo tudo orquestrado com Apache Airflow via Docker Compose.

---

##  ğŸ§° Tecnologias e Ferramentas Utilizadas

- Python 3.10+ â€“ Scripts ETL e API

- Apache Airflow â€“ OrquestraÃ§Ã£o de pipelines

- Docker & Docker Compose â€“ ContainerizaÃ§Ã£o do ambiente

- PostgreSQL â€“ Banco de dados relacional

- FastAPI â€“ ConstruÃ§Ã£o de APIs REST

- Pandas â€“ ManipulaÃ§Ã£o de dados

- SQL â€“ CriaÃ§Ã£o de tabelas e views analÃ­ticas

- SQLAlchemy / psycopg2 â€“ ConexÃ£o com o banco de dados

- dotenv â€“ Gerenciamento de variÃ¡veis de ambiente

- Linux / Ubuntu â€“ Ambiente de execuÃ§Ã£o

- Git â€“ Controle de versÃ£o


## ğŸ—‚ï¸ Estrutura do Projeto

```
.
â”œâ”€â”€ api/                       # API para consulta de dados analÃ­ticos
â”‚   â””â”€â”€ app.py                # FastAPI com endpoints REST
â”‚
â”œâ”€â”€ projeto/                  
â”‚   â”œâ”€â”€ dags/                 # DAGs do Airflow
â”‚   â”‚   â””â”€â”€ dag_etl.py
â”‚   â”œâ”€â”€ data/                 # Dados brutos (.csv)
â”‚   â”œâ”€â”€ etl/                  # Scripts de ETL em Python
â”‚   â”œâ”€â”€ logs/                 # Logs e metadados do Airflow
â”‚   â”œâ”€â”€ plugins/              # Plugins do Airflow (se necessÃ¡rio)
â”‚   â”œâ”€â”€ sql/                  # Scripts SQL de criaÃ§Ã£o de tabelas e views
â”‚   â”œâ”€â”€ .env                  # VariÃ¡veis de ambiente
â”‚   â””â”€â”€ docker-compose.yaml  # Arquitetura do ambiente
```

---

## ğŸ”„ Pipeline de Dados

1. IngestÃ£o de Dados  
   Arquivos `.csv` da pasta `data/` sÃ£o lidos e preparados para ingestÃ£o.

2. TransformaÃ§Ã£o e Limpeza  
   A lÃ³gica de transformaÃ§Ã£o estÃ¡ implementada nos scripts Python dentro da pasta `etl/`.

3. CriaÃ§Ã£o das Tabelas  
   As tabelas do schema `transacional` sÃ£o criadas com o script SQL `create_tables.sql`.

4. Carga de Dados  
   Os dados tratados sÃ£o inseridos no banco de dados RZK, no schema `transacional`.

5. CriaÃ§Ã£o das Views AnalÃ­ticas  
   As anÃ¡lises sÃ£o feitas via SQL em `create_views.sql`, criando views no schema `analitico`.

6. OrquestraÃ§Ã£o com Airflow  
   Todas as etapas acima sÃ£o automatizadas com o Apache Airflow, que roda via Docker Compose.

---

## ğŸŒ API

A API (fora do Docker Compose) Ã© construÃ­da com FastAPI e permite consultar:

- Dados da tabela `empresas`
- Views analÃ­ticas com os lucros por mÃªs

### Exemplos de Endpoints:

GET /empresas â†’ Lista todas as empresas
GET /empresas/{id} â†’ Detalhes de uma empresa especÃ­fica
GET /analitico/evolucao_receita
GET /analitico/gastos_salarios
GET /analitico/lucro_mes
GET /analitico/maiores_despesas
GET /analitico/melhores_clientes
GET /analitico/orcamento_consolidado
GET /analitico/transferencias_mensal

---

## ğŸ³ Docker Compose

O ambiente com Airflow estÃ¡ contido em um `docker-compose.yaml`, responsÃ¡vel por levantar:

- Scheduler
- Webserver
- Banco de dados
- Metadatabase do Airflow



## ğŸ› ï¸ Executando o Projeto

### Subir o Airflow com Docker Compose

```bash
cd projeto/
docker-compose up --build
```

### Rodar a API

A API estÃ¡ fora do Docker, entÃ£o rode localmente Executando o arquivo app.py:




## ğŸ‘¨â€ğŸ’» Autor

Projeto desenvolvido por JONATHAN NOVAIS
